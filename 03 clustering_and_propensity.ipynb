{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Acquisition for Clustering and Propensity Modeling\n",
    "\n",
    "### Overview\n",
    "In this section, we start our analysis by fetching the data necessary for clustering and propensity modeling. Utilizing Adobe's Customer Journey Analytics (CJA) API, we aim to pull a dataset within the date range of January 1, 2024, to January 31, 2024. This dataset will comprise crucial metrics such as:\n",
    "- Orders\n",
    "- Revenue\n",
    "- Visits\n",
    "- Occurrences\n",
    "- Time Spent\n",
    "\n",
    "These metrics are key to understanding user behavior on the website and will serve as the foundation for our clustering and propensity modeling.\n",
    "\n",
    "### Implementation Steps\n",
    "1. **Configuration and Library Importation**: Import necessary libraries and configure the CJA API.\n",
    "2. **Defining the Report Request**: Specify the data view ID, dimensions, metrics, and the global filter for the date range.\n",
    "3. **Data Retrieval and DataFrame Creation**: Execute the report request and load the data into a DataFrame for analysis.\n",
    "4. **Preliminary Data Review**: Display the first few rows of the DataFrame to ensure data has been successfully retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations and Imports\n",
    "import cjapy\n",
    "cjapy.importConfigFile(\"python_config.json\")\n",
    "\n",
    "# Initialize cjapy, data view, and date range\n",
    "cja = cjapy.CJA()\n",
    "data_view = \"dv_62ba17d5a5d7845496f5fb4d\"\n",
    "dateRange = \"2024-01-01T00:00:00.000/2024-01-31T00:00:00.000\"\n",
    "\n",
    "# Define the report request with selected metrics and dimensions\n",
    "myRequest = cjapy.RequestCreator()\n",
    "myRequest.setDataViewId(data_view)\n",
    "myRequest.setDimension(\"variables/adobe_personid\")\n",
    "myRequest.addMetric(\"metrics/orders\")\n",
    "myRequest.addMetric(\"metrics/revenue\")\n",
    "myRequest.addMetric(\"metrics/visits\")\n",
    "myRequest.addMetric(\"metrics/occurrences\")\n",
    "myRequest.addMetric(\"metrics/adobe_timespent\")\n",
    "myRequest.addGlobalFilter(dateRange)\n",
    "\n",
    "# Execute the report request and load the data into a DataFrame\n",
    "myReport = cja.getReport(myRequest)\n",
    "df = myReport.dataframe\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify successful data retrieval\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Clustering Model Preparation with t-SNE\n",
    "\n",
    "### Introduction to Clustering Techniques\n",
    "\n",
    "Exploring the vast landscape of clustering techniques unveils a plethora of methods designed to uncover natural groupings in data. These techniques are invaluable for identifying similarities among data points and segmenting data into meaningful clusters. Among the array of methods, our focus will be on a dimensionality reduction technique named t-SNE (t-Distributed Stochastic Neighbor Embedding), renowned for its efficacy in visualizing high-dimensional data in a more comprehensible lower-dimensional space.\n",
    "\n",
    "For further reading on clustering techniques, this [Northwestern University resource](https://sites.northwestern.edu/researchcomputing/2022/03/14/online-learning-resources-clustering/) offers a comprehensive overview.\n",
    "\n",
    "### Why t-SNE?\n",
    "\n",
    "t-SNE excels in transforming high-dimensional data into a 2D or 3D space, making it an exceptional tool for visualizing complex datasets. By applying t-SNE, we can:\n",
    "- **Visualize high-dimensional data** in a lower-dimensional space, enhancing interpretability.\n",
    "- **Identify natural clusters** within the data, based on the similarity of data points.\n",
    "\n",
    "### Implementing t-SNE for Website Visitor Data\n",
    "\n",
    "Our analysis applies t-SNE to website visitor data, aiming to visualize user behaviors and interactions with the website in a two-dimensional space. This visualization assists in identifying clusters of similar visitor behaviors, facilitating a deeper understanding of user engagement patterns and interests on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Drop the 'personid' column to create a DataFrame with only numerical columns\n",
    "df = myReport.dataframe.drop(columns=[\"itemId\"])\n",
    "X = myReport.dataframe.drop(columns=['variables/adobe_personid', \"itemId\"])\n",
    "\n",
    "# Create t-SNE instance\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "# Apply t-SNE\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Convert the t-SNE results to a DataFrame\n",
    "df_tsne = pd.DataFrame(X_tsne, columns=['tsne_1', 'tsne_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Visualizing t-SNE Results\n",
    "\n",
    "Having applied t-SNE to our website visitor data, we now move to visualize the transformed data. The t-SNE algorithm has reduced our high-dimensional dataset into a two-dimensional space, aiming to preserve the relative distances and relationships among data points. This visualization enables us to observe the clustering of similar behaviors, offering insights into how users interact with the website.\n",
    "\n",
    "### Visualization Objectives\n",
    "Our primary goal with this visualization is to:\n",
    "- Identify clusters of similar visitor behaviors.\n",
    "- Understand the distribution and grouping of data points in the 2D space.\n",
    "- Detect any outliers or unique patterns that emerge from the visualization.\n",
    "\n",
    "This step is crucial for our exploratory data analysis, providing a foundation for more detailed cluster analysis and interpretation in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "fig = px.scatter(df_tsne, x='tsne_1', y='tsne_2', title='t-SNE Visualization')\n",
    "\n",
    "# Equal aspect ratio ensures that one unit in the x-axis is equal to one unit in the y-axis\n",
    "fig.update_xaxes(scaleanchor=\"y\", scaleratio=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "# Adjust the layout for better readability\n",
    "fig.update_layout(height=600)\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Enhancing Clustering with DBSCAN\n",
    "\n",
    "After mapping the website visitors' behaviors into a visual and understandable format using t-SNE, we further refine our analysis by applying DBSCAN (Density-Based Spatial Clustering of Applications with Noise). This clustering technique is particularly effective for identifying high-density clusters and distinguishing outliers, providing a nuanced understanding of visitor groupings.\n",
    "\n",
    "### Why DBSCAN?\n",
    "\n",
    "DBSCAN stands apart due to its ability to form clusters based on density, making it adept at handling:\n",
    "- **Variably shaped clusters**: Unlike k-means, DBSCAN does not assume clusters to be spherical.\n",
    "- **Noise and outliers**: DBSCAN can identify and separate outliers from core clusters.\n",
    "\n",
    "By integrating DBSCAN with our t-SNE results, we aim to:\n",
    "- **Identify core clusters** of similar visitor behaviors.\n",
    "- **Detect outliers** that do not fit into any primary cluster.\n",
    "- **Understand the spatial distribution** of clusters and outliers in the 2D t-SNE space.\n",
    "\n",
    "This step is crucial for our clustering analysis, offering a comprehensive view of visitor behaviors and interactions on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Create DBSCAN instance with customized parameters\n",
    "dbscan = DBSCAN(eps=4.0, min_samples=20)\n",
    "\n",
    "# Fit DBSCAN to the t-SNE results\n",
    "dbscan.fit(df_tsne)\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "df['cluster'] = dbscan.labels_\n",
    "\n",
    "# Convert the 'cluster' column to a categorical type for better visualization\n",
    "df['cluster'] = df['cluster'].astype('category')\n",
    "df = pd.concat([df, df_tsne], axis=1)\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Create the scatter plot with DBSCAN clusters\n",
    "fig = px.scatter(df, x='tsne_1', y='tsne_2', color='cluster', title='t-SNE Visualization with DBSCAN Clusters')\n",
    "\n",
    "# Ensure equal aspect ratio for accurate representation\n",
    "fig.update_xaxes(scaleanchor=\"y\", scaleratio=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "# Adjust the layout for clarity\n",
    "fig.update_layout(height=600)\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Enhancing Analysis with 3D Visualization\n",
    "\n",
    "Following our exploration through t-SNE and DBSCAN clustering, we extend our analysis by incorporating a three-dimensional (3D) plot. This approach allows us to integrate an additional metric, such as revenue, to examine how these clusters perform concerning a specific metric of interest.\n",
    "\n",
    "### Objectives of 3D Visualization\n",
    "- **Depth of Insight**: By adding a third dimension, we can uncover patterns and distinctions not visible in 2D visualizations.\n",
    "- **Metric Integration**: Incorporating metrics like revenue enables us to identify high-value clusters or behaviors.\n",
    "- **Interactive Exploration**: A 3D plot allows users to rotate and zoom, providing a dynamic way to explore the data.\n",
    "\n",
    "This visualization not only serves to enhance our understanding of the clusters formed but also invites us to investigate how specific metrics vary across different clusters, offering a comprehensive view of user behavior and its impact on revenue generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3D scatter plot\n",
    "fig = px.scatter_3d(df, x='tsne_1', y='tsne_2', z='metrics/revenue', color='cluster', title='3D t-SNE Visualization with DBSCAN Clusters')\n",
    "\n",
    "# Adjust the layout for a better viewing experience\n",
    "fig.update_layout(height=700)\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Building a Propensity Model\n",
    "\n",
    "With the clustering analysis providing insights into user behaviors and groupings, our next step is to leverage this understanding to predict future actions. Specifically, we aim to build a propensity model to identify users who, although have not yet made a purchase, show a high likelihood of converting in the future.\n",
    "\n",
    "### Objectives of the Propensity Model\n",
    "- **Predictive Analysis**: Utilize historical data to predict the probability of users making a purchase.\n",
    "- **Targeting Strategy**: Enable targeted marketing strategies by identifying users with high conversion potential.\n",
    "- **Data Preparation**: Format and prepare our dataset, focusing on metrics that contribute to purchase propensity without directly revealing purchase information.\n",
    "\n",
    "This model will help us not only in understanding current user behaviors but also in forecasting future actions, allowing for more informed decision-making and strategic planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Convert 'metrics/orders' into a binary target variable for whether a purchase was made\n",
    "df['purchase_made'] = (df['metrics/orders'] > 0).astype(int)\n",
    "\n",
    "# Prepare the dataset for modeling by dropping columns that might lead to data leakage\n",
    "X_propensity = df.drop(columns=['metrics/orders', 'metrics/revenue', 'variables/adobe_personid', 'cluster', 'tsne_1', 'tsne_2', 'purchase_made'])\n",
    "y_propensity = df['purchase_made']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_propensity, y_propensity, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Display the features and target variable to ensure proper data preparation\n",
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Model Evaluation Metrics\n",
    "\n",
    "After training our logistic regression model to predict the likelihood of a website visitor making a purchase, we evaluate the model's performance using several key metrics. These metrics provide insight into how well our model performs across different aspects such as accuracy, precision, recall, and the ability to distinguish between visitors who will and will not make a purchase.\n",
    "\n",
    "### Key Evaluation Metrics\n",
    "- **Accuracy**: Measures the proportion of total predictions that are correct.\n",
    "- **ROC AUC**: Reflects the model's ability to discriminate between positive and negative classes.\n",
    "- **Classification Report**: Includes precision, recall, and F1-score for a detailed performance analysis.\n",
    "\n",
    "Understanding these metrics is crucial for interpreting the model's effectiveness and identifying areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, roc_curve, auc, confusion_matrix\n",
    "\n",
    "# Predict on the test set and calculate evaluation metrics\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "y_proba = logistic_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calculate and normalize confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "conf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Visualization setup\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('ROC Curve', 'Normalized Confusion Matrix'), horizontal_spacing=0.2)\n",
    "\n",
    "# ROC Curve\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC curve (AUC = {roc_auc:.2f})', line=dict(color='darkorange')), row=1, col=1)\n",
    "fig.add_shape(type='line', line=dict(dash='dash', color='navy'), x0=0, x1=1, y0=0, y1=1, row=1, col=1)\n",
    "\n",
    "# Normalized Confusion Matrix\n",
    "conf_mat_fig = ff.create_annotated_heatmap(z=conf_mat_normalized, x=['Not Purchased', 'Purchased'], y=['Not Purchased', 'Purchased'],\n",
    "                                           colorscale='Blues', annotation_text=np.round(conf_mat_normalized, 2), showscale=True)\n",
    "for trace in conf_mat_fig.data:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "# Final adjustments and display\n",
    "fig.update_layout(title_text='Model Evaluation: ROC Curve and Normalized Confusion Matrix', width=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Identifying High Propensity Users\n",
    "\n",
    "With our propensity model in place, the next step is to apply this model to our dataset to identify users who have not yet made a purchase but show a high likelihood of doing so in the future. This step is crucial for targeting and engagement strategies, allowing us to focus our efforts on users most likely to convert.\n",
    "\n",
    "### Objectives\n",
    "- **Application of the Propensity Model**: Use the model to predict purchase probabilities for all users.\n",
    "- **Filtering High Propensity Users**: Identify users with a high probability of making a purchase, focusing our marketing and engagement strategies on this group.\n",
    "- **Visualization**: Create a histogram to visualize the distribution of purchase probabilities, helping us understand the overall propensity landscape of our user base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities for the entire dataset\n",
    "purchase_probabilities = logistic_model.predict_proba(X_propensity)\n",
    "\n",
    "# Extract the probabilities of making a purchase\n",
    "probabilities_of_purchase = purchase_probabilities[:, 1]\n",
    "\n",
    "# Initialize the 'purchase_probability' column with NaNs or zeros\n",
    "df['purchase_probability'] = np.nan\n",
    "\n",
    "# Update 'purchase_probability' only for rows where no purchase has been made\n",
    "df.loc[df['metrics/orders'] == 0, 'purchase_probability'] = probabilities_of_purchase[df['metrics/orders'] == 0]\n",
    "\n",
    "# Filter out NaN values from 'purchase_probability' to focus on non-purchased users\n",
    "filtered_probabilities = df['purchase_probability'].dropna()\n",
    "\n",
    "# Create a histogram/distribution plot with 4 larger buckets between 0 and 100%\n",
    "fig = px.histogram(filtered_probabilities, range_x=[0, 1], \n",
    "                   labels={'value': 'Purchase Probability'},\n",
    "                   title='Distribution of Purchase Probabilities for Users Who Have Not Yet Purchased')\n",
    "\n",
    "# Calculate the count of visitors in each bin\n",
    "bin_counts = pd.cut(filtered_probabilities, bins=np.linspace(0, 1, 5)).value_counts().sort_index()\n",
    "\n",
    "# Update the number of bins and adjust the bin edges\n",
    "fig.update_traces(xbins=dict(\n",
    "        start=0,\n",
    "        end=1,\n",
    "        size=0.25\n",
    "    ))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Exporting Data for Further Analysis and Targeting\n",
    "\n",
    "The final step in our analysis involves exporting the cluster assignments and propensity scores into a JSON file. This file can then be uploaded to Adobe Experience Platform (AEP) for detailed analysis in Analysis Workspace, or used in Customer Data Platforms (CDP) for targeted audience engagement. Exporting this data allows us to operationalize our findings, applying the insights gained from clustering and propensity modeling to enhance marketing strategies and customer experiences.\n",
    "\n",
    "### Objectives\n",
    "- **Data Export**: Compile the cluster assignments and propensity scores into a structured JSON format.\n",
    "- **Integration with AEP and CDP**: Enable seamless integration of our analytical outcomes with marketing platforms for actionable insights.\n",
    "- **Operationalization of Insights**: Leverage the exported data for targeted marketing campaigns, personalized customer engagement, and strategic decision-making.\n",
    "\n",
    "### Preparing for Data Export\n",
    "Before exporting your data, it's essential to obtain your **tenant ID** from AEP, which serves as a prefix for loading data into the platform. Your tenant ID is readily available within the AEP Schemas UI when creating a new schema. Alternatively, for a detailed guide on locating your tenant ID and understanding the foundational elements of AEP, refer to Adobe's documentation: [Getting Started with Adobe Experience Platform](https://experienceleague.adobe.com/docs/experience-platform/xdm/api/getting-started.html?lang=en).\n",
    "\n",
    "By completing this step, we bridge the gap between data analysis and practical application, ensuring that the valuable insights derived from our models are readily accessible for strategic initiatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = \"cluster_and_propensities.json\"\n",
    "\n",
    "# Prepare data for export\n",
    "data_to_export = []\n",
    "for index, row in df.iterrows():\n",
    "    # Initialize the JSON object structure\n",
    "    json_object = {\n",
    "        \"_tenantid\": {\n",
    "            \"personID\": row['variables/adobe_personid'],\n",
    "            \"clusterid\": int(row['cluster'])  # Ensure cluster ID is integer\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add propensity score if available and not NaN\n",
    "    propensity = row.get('purchase_probability')\n",
    "    if pd.notnull(propensity):\n",
    "        # Make sure to replace this with your company's actual tenant id!\n",
    "        json_object[\"_tenantid\"][\"propensity\"] = round(propensity, 2)\n",
    "    \n",
    "    # Append the JSON object to the list\n",
    "    data_to_export.append(json_object)\n",
    "\n",
    "# Write data to a JSON Lines file\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for item in data_to_export:\n",
    "        file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"Data successfully exported to {output_file_path} - note that users who have previously made a purchase will not have a propensity score in the output.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
